{"cells":[{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"DDJwQPZcupab","new_sheet":false,"run_control":{"read_only":false}},"source":["# CS 6476 Assignment 2 | Part 1: Fully-Connected Neural Networks for Image Classification"]},{"cell_type":"markdown","metadata":{"id":"3JwgQ7rWA-EL"},"source":["# AIM\n"," The aim of this assignment is to implement a fully-connected neural network from scratch on CIFAR-10 image classification dataset."]},{"cell_type":"markdown","metadata":{"id":"IENr0eT0A-EL"},"source":["# Summary\n","The goal of this part of the assignment is to provide hands-on experience with coding fully connected neural networks from scratch in pytorch. The structure of the coding part of the assignment is modular to iterate over the components involved. Finally we will use, optimize and train our final model on CIFAR-10 image classification dataset, assess the accuracy of our model and answer some questions regarding the same. For you understanding certain blogs have been provided in the notebook. We believe they can help you better understand concepts that you are about to implement. More such resources for reading will be provided over ed-discussions.\n","## Learning Objectives\n","- First Prinicipal coding of FC-NN\n","- Develope basic understanding of components of an end-end FC-NN\n","- Develope basic uderstanding of regularization techniques and model optimizers\n","- Develope basic understanding on how to fine tune a model using validation set\n","\n","\n","\n","## Grading Schema\n","This part of the assignment will be graded out of 100. A good representative of the testcases are already provided in the notebook.\n","- Linear Layer (10)\n","- ReLU Layer (10)\n","- Convinence Module (10)\n","- Two-Layer-Network (15)\n","- Multi-Layer-Network (15)\n","- Momentum Optimizer (10)\n","- Dropout (10)\n","- Optimal Fine Tuned Model (20)\n","\n","\n","## Time Commitement\n","This part of the assignment should take about 6 hrs of effort to finish. Half of the time estimate is for the associated readings to get a better understanding of the concepts and how it can be coded.\n","\n","\n","\n","## Computational Requirements\n","This part of the assignment should not take more than 20 mins in total to execute each part, including training of the optimal model. These estimates are for code running on CPU. Students can see significantly faster results if running on GPU. Refer to `google colab` setup to see how you can get free access to GPU to execute the assignment if you wish to. (note it is not needed, and CPU execution executes in reasonable time and wont effect your final output or grades)."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"Q7ymI0aZ2W1b","new_sheet":false,"run_control":{"read_only":false}},"source":["# Setup\n","If running on local system install the packages through following commands.\n","```\n","pip install opencv-python\n","pip install torchvision\n","pip install torch\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"DW4zHQ52A-EL"},"source":["## Colab Setup\n","First, run the following cell to load the \"autoreload\" extension. The \"autoreload\" extension allows you to automatically reload (re-import) Python modules that you've imported or defined when they change. This is particularly useful when you are actively developing or modifying code in external modules and want those changes to be automatically reflected in your notebook without manually restarting the kernel. Those not running colab avoid running the cells associated to `colab setup`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5zJZRLaA-EM"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"bPvfSONWA-EM"},"source":["Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59mWuwVwA-EM"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"CUeLuApqA-EM"},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-_a_Hz7A-EM"},"outputs":[],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a CV2023 folder and put all the files under A2 folder, then 'CV2023/A2'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None #enter file name as the above example\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"markdown","metadata":{"id":"51LM5JWAA-EM"},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the .py files of this part of the assignment. If it works correctly, it should print the message:\n","```\n","Hello from fully_connected_neural_networks.py!\n","Hello from a2_helper.py!\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tj4P9vp8A-EN"},"outputs":[],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()\n","\n","from fully_connected_networks import hello_fully_connected_networks\n","hello_fully_connected_networks()\n","\n","from a2_helper import hello_helper\n","hello_helper()\n","\n","FC_networks_path = os.path.join(GOOGLE_DRIVE_PATH, 'fully_connected_networks.py')\n","FC_networks_edit_time = time.ctime(os.path.getmtime(FC_networks_path))\n","print('fully_connected_networks.py last edited on %s' % FC_networks_edit_time)"]},{"cell_type":"markdown","metadata":{"id":"HKJXA5mSA-EN"},"source":["On colab install dependecies using:\n","```\n","!pip install opencv-python\n","!pip install torchvision\n","!pip install torch\n","```\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"fN1SShPR4lJV","new_sheet":false,"run_control":{"read_only":false}},"source":["# Imports\n","Import requisit packages to work with the provided files for part-1 of the assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"VUCKw4Tl1ddj","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import cs6476\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from cs6476 import Solver\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","plt.rcParams['font.size'] = 16"]},{"cell_type":"code","source":["DEVICE = 'cpu'"],"metadata":{"id":"sI200m6RepnF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"id":"gkJDmaMfjpWV"},"source":["Run this cell to check if you are using a GPU. Don't use GPU if you want to save on your daily GPU usage. We recommend you to save it for the Part 2."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"id":"sv3mLOGijpWV"},"outputs":[],"source":["if torch.cuda.is_available():\n","  DEVICE = 'cuda'\n","  print('Good to go!')\n","else:\n","  print('If you want to use GPU then set it via Edit -> Notebook Settings -> Hardware accelerator -> T4 GPU')"]},{"cell_type":"markdown","source":["IMPORTANT: If you have reached the GPU usage limit then 'connect without GPU' and change the DEVICE variable in the below cell to 'cpu'. Else make sure to set DEVICE to 'cuda'"],"metadata":{"id":"5aA5lPAhQ2zy"}},{"cell_type":"code","source":["# DEVICE = 'cpu'\n","# uncomment above if GPU is not avaible\n","\n","# DEVICE = 'cuda'\n","# uncomment above if gpu is avaible"],"metadata":{"id":"sFJf78N8RF7k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ynKS05gJ4iBo","new_sheet":false,"run_control":{"read_only":false}},"source":["# Data preprocessing"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"-Yv3zQYw5B3s","new_sheet":false,"run_control":{"read_only":false}},"source":["## Load and Visualize the CIFAR-10 dataset\n","We will first load the CIFAR-10 dataset. The utility function `cs6476.data.preprocess_cifar10()` returns the entire CIFAR-10 dataset as a set of six **Torch tensors** while also preprocessing the RGB images. We then split the data into 3 components, Train, Validate and Test.\n","\n","- `X_train` contains all training images (real numbers in the range $[0, 1]$)\n","- `y_train` contains all training labels (integers in the range $[0, 9]$)\n","- `X_val` contains all validation images\n","- `y_val` contains all validation labels\n","- Note: The seed value should remain untouched while coding your experiments, to ensure consistency in results on gradescope.\n","\n","- To know more about CIFAR datasets read https://www.cs.toronto.edu/~kriz/cifar.html\n","- Please note that you have to use your `gtid` to set seed for all experiments and code. It will be crucial for accurate testing over gradescope"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"V2mFlFmQ1ddm","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["cs6476.utils.reset_seed(6476)\n","data_dict = cs6476.data.preprocess_cifar10(cuda=False, dtype=torch.float64)\n","print('Train data shape: ', data_dict['X_train'].shape)\n","print('Train labels shape: ', data_dict['y_train'].shape)\n","print('Validation data shape: ', data_dict['X_val'].shape)\n","print('Validation labels shape: ', data_dict['y_val'].shape)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"ZeH0OvuEe1CN","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-title"]},"source":["# Fully-connected neural networks"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3Qiu9_4pe1CP","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-ignore"]},"source":["In this assignment we will implement fully-connected networks using a modular approach. First We will implement a forward and backward function such that we can easily combine a combination of them to build classifiers with different architectures. In addition to implementing fully-connected networks of arbitrary depth, we will also explore different rules for optimization, and Dropout as a regularizer.\n","\n","To validate our implementation, we have provided testing code for each module. The flow of the notebook is setup in such a fashion as to assist with the order of implementation and to debug code. We also provide visualization modules to test the performance of the model. Finally we ask the students to implment both a 2 layer Fully-Connected NN and an aribtrary depth Fully-Connected NN (with parameters of choice) to test the performance.\n","  "]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"JB7Eu3qJ9xnm","new_sheet":false,"run_control":{"read_only":false}},"source":["# Linear layer"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"bRdnxsvZunFu","new_sheet":false,"run_control":{"read_only":false}},"source":["For each layer we implement, we will define a class with two static methods `forward` and `backward` (already implemented). The class structure is currently provided in `fully_connected_layers.py`, you will be implementing both the `forward` and `backward` methods.\n","\n","You can read the following blog ot get a better understanding of the entire process: https://theneuralblog.com/forward-pass-backpropagation-example/"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"0NNv3l-ne1Cb","new_sheet":false,"run_control":{"read_only":false}},"source":["## Linear layer: forward\n","Test your implementation of `Linear.forward` using below code. You should see errors less than `1e-7`. Use Device as 'cuda' if you have access to GPU.\n","- you have to implement `out = XW + b`"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"sjq2Sq4Ze1Cc","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Linear\n","\n","# Test the Linear.forward function\n","num_inputs = 2\n","input_shape = torch.tensor((4, 5, 6))\n","output_dim = 3\n","\n","input_size = num_inputs * torch.prod(input_shape)\n","weight_size = output_dim * torch.prod(input_shape)\n","\n","x = torch.linspace(-0.1, 0.5, steps=input_size, dtype=torch.float64,device='cpu')\n","w = torch.linspace(-0.2, 0.3, steps=weight_size, dtype=torch.float64,device='cpu')\n","b = torch.linspace(-0.3, 0.1, steps=output_dim, dtype=torch.float64,device='cpu')\n","x = x.reshape(num_inputs, *input_shape)\n","w = w.reshape(torch.prod(input_shape), output_dim)\n","\n","out, _ = Linear.forward(x, w, b)\n","correct_out = torch.tensor([[1.49834984, 1.70660150, 1.91485316],\n","                            [3.25553226, 3.51413301, 3.77273372]]\n","                            ).double()#.cuda() // uncomment .cuda() if you have access to gpu\n","\n","print('Testing Linear.forward function:')\n","print('difference: ', cs6476.grad.rel_error(out, correct_out))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"4mxIDo46e1Cf","new_sheet":false,"run_control":{"read_only":false}},"source":["## Linear layer: backward\n","We have already provided the code for `Linear.backward`. Test the overall implementation using below code. You should see errors less than `1e-7`. Use Device as 'cuda' if you have access to GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"ts85gmote1Cg","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Linear\n","\n","# Test the Linear.backward function\n","cs6476.utils.reset_seed(6476)\n","x = torch.randn(10, 2, 3, dtype=torch.float64)#, device='cuda'\n","w = torch.randn(6, 5, dtype=torch.float64)\n","b = torch.randn(5, dtype=torch.float64)\n","dout = torch.randn(10, 5, dtype=torch.float64)\n","\n","dx_num = cs6476.grad.compute_numeric_gradient(lambda x: Linear.forward(x, w, b)[0], x, dout)\n","dw_num = cs6476.grad.compute_numeric_gradient(lambda w: Linear.forward(x, w, b)[0], w, dout)\n","db_num = cs6476.grad.compute_numeric_gradient(lambda b: Linear.forward(x, w, b)[0], b, dout)\n","\n","_, cache = Linear.forward(x, w, b)\n","dx, dw, db = Linear.backward(dout, cache)\n","\n","# The error should be around e-10 or less\n","print('Testing Linear.backward function:')\n","print('dx error: ', cs6476.grad.rel_error(dx_num, dx))\n","print('dw error: ', cs6476.grad.rel_error(dw_num, dw))\n","print('db error: ', cs6476.grad.rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"bdIqQzqiJQE6","new_sheet":false,"run_control":{"read_only":false}},"source":["# ReLU activation"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"YdX98A_qvTRt","new_sheet":false,"run_control":{"read_only":false}},"source":["We will now implement the ReLU nonlinearity. The class structure can be found in `fully_connected_networks.py`\n","\n","Brief Idea about ReLU and why it is a good activation function: https://www.mygreatlearning.com/blog/relu-activation-function/"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"n2DyqL4Ae1Cl","new_sheet":false,"run_control":{"read_only":false}},"source":["## ReLU activation: forward\n","Test your implementation of `ReLU.forward` using below code. You should see errors less than `1e-8`. Use Device as 'cuda' if you have access to GPU.\n","- you have to implement `out = max(0,x)`"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"QblpieUJe1Cm","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import ReLU\n","\n","cs6476.utils.reset_seed(6476)\n","x = torch.linspace(-0.5, 0.5, steps=12, dtype=torch.float64,device='cpu')\n","x = x.reshape(3, 4)\n","\n","out, _ = ReLU.forward(x)\n","correct_out = torch.tensor([[ 0.,          0.,          0.,          0.,        ],\n","                            [ 0.,          0.,          0.04545455,  0.13636364,],\n","                            [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]],\n","                            dtype=torch.float64,device='cpu')\n","\n","print('Testing ReLU.forward function:')\n","print('difference: ', cs6476.grad.rel_error(out, correct_out))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3bSInb7xe1Cq","new_sheet":false,"run_control":{"read_only":false}},"source":["## ReLU activation: backward\n","We have already provided the code for `ReLU.backward`. Test the implementation of `ReLU.backward` in your overall code flow using below code. You should see errors less than `1e-8`. Use Device as 'cuda' if you have access to GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"odiV48zBe1Cr","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import ReLU\n","\n","cs6476.utils.reset_seed(6476)\n","x = torch.randn(10, 10, dtype=torch.float64,device='cpu')\n","dout = torch.randn(*x.shape, dtype=torch.float64,device='cpu')\n","\n","dx_num = cs6476.grad.compute_numeric_gradient(lambda x: ReLU.forward(x)[0], x, dout)\n","\n","_, cache = ReLU.forward(x)\n","dx = ReLU.backward(dout, cache)\n","\n","print('Testing ReLU.backward function:')\n","print('dx error: ', cs6476.grad.rel_error(dx_num, dx))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"eVTMuUOZe1Cv","new_sheet":false,"run_control":{"read_only":false}},"source":["# \"Convinience\" layers\n","Linear layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define a convenience layer. Combination forward and backward linear layers in conjunction with ReLU is given in code `Linear_ReLU`. Run the following to verify you implementations in conjunction. You should see errors less than `1e-8`.Use Device as 'cuda' if you have access to GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"-gaY5YfAe1Cw","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Linear_ReLU\n","\n","cs6476.utils.reset_seed(6476)\n","x = torch.randn(2, 3, 4, dtype=torch.float64,device='cpu')\n","w = torch.randn(12, 10, dtype=torch.float64,device='cpu')\n","b = torch.randn(10, dtype=torch.float64,device='cpu')\n","dout = torch.randn(2, 10, dtype=torch.float64,device='cpu')\n","\n","out, cache = Linear_ReLU.forward(x, w, b)\n","dx, dw, db = Linear_ReLU.backward(dout, cache)\n","\n","dx_num = cs6476.grad.compute_numeric_gradient(lambda x: Linear_ReLU.forward(x, w, b)[0], x, dout)\n","dw_num = cs6476.grad.compute_numeric_gradient(lambda w: Linear_ReLU.forward(x, w, b)[0], w, dout)\n","db_num = cs6476.grad.compute_numeric_gradient(lambda b: Linear_ReLU.forward(x, w, b)[0], b, dout)\n","\n","# Relative error should be around e-8 or less\n","print('Testing Linear_ReLU.forward and Linear_ReLU.backward:')\n","print('dx error: ', cs6476.grad.rel_error(dx_num, dx))\n","print('dw error: ', cs6476.grad.rel_error(dw_num, dw))\n","print('db error: ', cs6476.grad.rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"rAGgiyP5e1C0","new_sheet":false,"run_control":{"read_only":false}},"source":["# Loss layers: Softmax\n","we have provided the softmax loss function to you for free in `helper_functions.py`.You cn verify your code flow so far by running the script below. You should see errors less than `1e-6` for softmax_loss."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"BU9xp64De1C1","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from a2_helper import softmax_loss\n","\n","cs6476.utils.reset_seed(6476)\n","num_classes, num_inputs = 10, 50\n","x = 0.001 * torch.randn(num_inputs, num_classes, dtype=torch.float64,device='cpu')\n","y = torch.randint(num_classes, size=(num_inputs,), dtype=torch.int64,device='cpu')\n","\n","dx_num = cs6476.grad.compute_numeric_gradient(lambda x: softmax_loss(x, y)[0], x)\n","loss, dx = softmax_loss(x, y)\n","\n","# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n","print('\\nTesting softmax_loss:')\n","print('loss: ', loss.item())\n","print('dx error: ', cs6476.grad.rel_error(dx_num, dx))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"qq7-cyfQe1C4","new_sheet":false,"run_control":{"read_only":false}},"source":["# Two-layer network\n","Utilize you modular implementation to build a Two-layer network. Complete the implementation in the `TwoLayerNet` class. Test your implementation using the following code.\n","Note: Backpropogation/Backward pass has already been implemented for your convinience.\n","Refer to the following formula when adding regularization term to the loss."]},{"cell_type":"markdown","source":["$regularized\\_loss = loss + \\lambda \\sum_{j}w_j^2$"],"metadata":{"id":"ciHzbEa2PFoZ"}},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"d3JOcfyze1C5","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import TwoLayerNet\n","from a2_helper import softmax_loss\n","\n","cs6476.utils.reset_seed(6476)\n","N, D, H, C = 3, 5, 50, 7\n","X = torch.randn(N, D, dtype=torch.float64,device='cpu')\n","y = torch.randint(C, size=(N,), dtype=torch.int64,device='cpu')\n","\n","std = 1e-3\n","model = TwoLayerNet(\n","          input_dim=D,\n","          hidden_dim=H,\n","          num_classes=C,\n","          weight_scale=std,\n","          dtype=torch.float64,\n","          device='cpu'\n","        )\n","\n","print('Testing initialization ... ')\n","W1_std = torch.abs(model.params['W1'].std() - std)\n","b1 = model.params['b1']\n","W2_std = torch.abs(model.params['W2'].std() - std)\n","b2 = model.params['b2']\n","assert W1_std < std / 10, 'First layer weights do not seem right'\n","assert torch.all(b1 == 0), 'First layer biases do not seem right'\n","assert W2_std < std / 10, 'Second layer weights do not seem right'\n","assert torch.all(b2 == 0), 'Second layer biases do not seem right'\n","\n","print('Testing test-time forward pass ... ')\n","model.params['W1'] = torch.linspace(-0.7, 0.3, steps=D * H, dtype=torch.float64,device='cpu').reshape(D, H)\n","model.params['b1'] = torch.linspace(-0.1, 0.9, steps=H, dtype=torch.float64,device='cpu')\n","model.params['W2'] = torch.linspace(-0.3, 0.4, steps=H * C, dtype=torch.float64,device='cpu').reshape(H, C)\n","model.params['b2'] = torch.linspace(-0.9, 0.1, steps=C, dtype=torch.float64,device='cpu')\n","X = torch.linspace(-5.5, 4.5, steps=N * D, dtype=torch.float64,device='cpu').reshape(D, N).t()\n","scores = model.loss(X)\n","correct_scores = torch.tensor(\n","  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n","   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n","   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]],\n","    dtype=torch.float64,device='cpu')\n","scores_diff = torch.abs(scores - correct_scores).sum()\n","assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n","\n","print('Testing training loss (no regularization)')\n","y = torch.tensor([0, 5, 1])\n","loss, grads = model.loss(X, y)\n","correct_loss = 3.4702243556\n","assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n","\n","model.reg = 1.0\n","loss, grads = model.loss(X, y)\n","correct_loss = 49.719461034881775\n","assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n","\n","# Errors should be around e-6 or less\n","for reg in [0.0, 0.7]:\n","  print('Running numeric gradient check with reg = ', reg)\n","  model.reg = reg\n","  loss, grads = model.loss(X, y)\n","\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = cs6476.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, cs6476.grad.rel_error(grad_num, grads[name])))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"q1Odj9XQe1C9","new_sheet":false,"run_control":{"read_only":false}},"source":["# Solver\n","We have split the logic for training models into a separate class. We use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set. For the two layer net we use a specific instance of the function. Use the helper function to get a general idea on how to use `Solver`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVo5gjegA-ES"},"outputs":[],"source":["print(help(Solver))"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"6unJrOule1C_","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import create_solver_instance\n","\n","cs6476.utils.reset_seed(6476)\n","\n","# Create a solver instance that achieves 50% performance on the validation set\n","solver = create_solver_instance(data_dict=data_dict, dtype=torch.float64,device='cpu')\n","solver.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"gSSy7LTde1DE","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Run this cell to visualize training loss and train / val accuracy\n","plt.subplot(2, 1, 1)\n","plt.title('Training loss')\n","plt.plot(solver.loss_history, 'o')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(2, 1, 2)\n","plt.title('Accuracy')\n","plt.plot(solver.train_acc_history, '-o', label='train')\n","plt.plot(solver.val_acc_history, '-o', label='val')\n","plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n","plt.xlabel('Epoch')\n","plt.legend(loc='lower right')\n","plt.gcf().set_size_inches(15, 12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"eNyFLT1We1DI","new_sheet":false,"run_control":{"read_only":false}},"source":["# Multilayer network\n","On the lines of Two-layer Fully-Connected NN, you have to now implement a Fully-Connected NN which can create model architecture of arbitrary depth. You can ignore incorporating drop out into the code for now.\n","\n","Read through the `FullyConnectedNet` class in `fully_connected_networks.py`.\n","\n","Note: Backpropogation/Backward pass has already been implemented for your convinience."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"3abR1_qhe1DK","new_sheet":false,"run_control":{"read_only":false}},"source":["## Initial loss and gradient check\n","\n","As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n","\n","For gradient checking, you should expect to see errors less than `1e-6`, except for the check on `W1` and `W2` with `reg=0` where your errors should be less than `1e-5`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"1waPtKRDe1DL","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet\n","\n","cs6476.utils.reset_seed(6476)\n","N, D, H1, H2, C = 2, 15, 20, 30, 10\n","X = torch.randn(N, D, dtype=torch.float64, device='cpu')\n","y = torch.randint(C, size=(N,), dtype=torch.int64, device='cpu')\n","\n","for reg in [0, 3.14]:\n","  print('Running check with reg = ', reg)\n","  model = FullyConnectedNet(\n","        [H1, H2],\n","        input_dim=D,\n","        num_classes=C,\n","        reg=reg,\n","        weight_scale=5e-2,\n","        dtype=torch.float64,\n","        device='cpu'\n","  )\n","\n","  loss, grads = model.loss(X, y)\n","  print('Initial loss: ', loss.item())\n","\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = cs6476.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, cs6476.grad.rel_error(grad_num, grads[name])))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"T4eWrnY7e1Di","new_sheet":false,"run_control":{"read_only":false}},"source":["# Update rules\n","So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement one of the most commonly used update rules and compare it to vanilla SGD."]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"zBDJqbeVe1Dn","new_sheet":false,"run_control":{"read_only":false}},"source":["## SGD+Momentum\n","Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n","\n","We provide the implementation of the SGD update rule for your reference in `fully_connected_networks.py`\n","\n","Now **implement** the SGD+Momentum update rule using the same interface. Run the following to check your implementation of SGD+Momentum. You should see errors less than `1e-7`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"RbQrkNo_e1Dp","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import sgd_momentum\n","\n","cs6476.utils.reset_seed(6476)\n","\n","N, D = 4, 5\n","w = torch.linspace(-0.4, 0.6, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n","dw = torch.linspace(-0.6, 0.4, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n","v = torch.linspace(0.6, 0.9, steps=N*D, dtype=torch.float64, device='cpu').reshape(N, D)\n","\n","config = {'learning_rate': 1e-3, 'velocity': v}\n","next_w, _ = sgd_momentum(w, dw, config=config)\n","\n","expected_next_w = torch.tensor([\n","  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n","  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n","  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n","  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]],\n","   dtype=torch.float64)#, device='cuda'\n","expected_velocity = torch.tensor([\n","  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n","  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n","  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n","  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]],\n","   dtype=torch.float64, device='cpu')\n","\n","# Should see relative errors around e-8 or less\n","print('next_w error: ', cs6476.grad.rel_error(next_w, expected_next_w))\n","print('velocity error: ', cs6476.grad.rel_error(expected_velocity, config['velocity']))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"7QQj73zje1D2","new_sheet":false,"run_control":{"read_only":false}},"source":["Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"qXdMNC9Ve1D4","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet, sgd, sgd_momentum,adam\n","\n","# TODO: Use a three-layer Net to overfit 50 training examples by\n","# tweaking just the learning rate and initialization scale.\n","cs6476.utils.reset_seed(6476)\n","\n","num_train = 4000\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","solvers = {}\n","\n","for update_rule_name, update_rule_fn in [('sgd', sgd), ('sgd_momentum', sgd_momentum)]:\n","  print('running with ', update_rule_name)\n","  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2,\n","                            dtype=torch.float32)#, device='cuda'\n","\n","  solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule_fn,\n","                  optim_config={\n","                    'learning_rate': 5e-2,\n","                  },\n","                  print_every=1000,\n","                  verbose=True, device='cpu')#\n","  solvers[update_rule_name] = solver\n","  solver.train()\n","  print()\n","\n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","for update_rule, solver in solvers.items():\n","  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","\n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","for update_rule, solver in solvers.items():\n","  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","\n","\n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","for update_rule, solver in solvers.items():\n","  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n","plt.legend(loc='lower center', ncol=4)\n","\n","plt.gcf().set_size_inches(10, 20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"C2_BL-2TwxKR","new_sheet":false,"run_control":{"read_only":false},"tags":["pdf-title"]},"source":["# Dropout\n","Dropout [1] is a technique for regularizing neural networks by randomly setting some output activations to zero during the forward pass. You will implement a dropout layer, provisions for which already exist in fully-connected network to optionally use dropout.\n","\n","[1] [Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012](https://arxiv.org/abs/1207.0580)\n","\n","Another blog with code to understand dropout: https://medium.com/analytics-vidhya/a-simple-introduction-to-dropout-regularization-with-code-5279489dda1e"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"s68cb0QBwxKj","new_sheet":false,"run_control":{"read_only":false}},"source":["## Dropout: forward\n","**Implement** the forward pass for dropout in `fully_connected_networks.py`. Since dropout behaves differently during training and testing, make sure to implement the operation for both modes. Adjust your code in `FullyConnectedNet` to incorporate dropout.\n","\n","Run the following to test your dropout implementation. The mean of the output should be approximately the same during training and testing. During training the number of outputs set to zero should be approximately equal to the drop probability `p`, and during testing no outputs should be set to zero."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"vFAmI9VxwxKk","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Dropout\n","\n","cs6476.utils.reset_seed(6476)\n","x = torch.randn(500, 500, dtype=torch.float64, device='cpu') + 10\n","\n","for p in [0.25, 0.4, 0.7]:\n","  out, _ = Dropout.forward(x, {'mode': 'train', 'p': p})\n","  out_test, _ = Dropout.forward(x, {'mode': 'test', 'p': p})\n","\n","  print('Running tests with p = ', p)\n","  print('Mean of input: ', x.mean().item())\n","  print('Mean of train-time output: ', out.mean().item())\n","  print('Mean of test-time output: ', out_test.mean().item())\n","  print('Fraction of train-time output set to zero: ', (out == 0).type(torch.float32).mean().item())\n","  print('Fraction of test-time output set to zero: ', (out_test == 0).type(torch.float32).mean().item())\n","  print()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"dt2BpwxswxKn","new_sheet":false,"run_control":{"read_only":false}},"source":["## Dropout: backward\n","Note: Backpropogation/Backward pass has already been implemented for your convinience. You can just run the below cell to test the overall implementation of the flow."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"3uctLwyIwxKo","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import Dropout\n","\n","cs6476.utils.reset_seed(6476)\n","x = torch.randn(10, 10, dtype=torch.float64, device='cpu') + 10\n","dout = torch.randn_like(x)\n","\n","dropout_param = {'mode': 'train', 'p': 0.2, 'seed': 0}\n","out, cache = Dropout.forward(x, dropout_param)\n","dx = Dropout.backward(dout, cache)\n","dx_num = cs6476.grad.compute_numeric_gradient(lambda xx: Dropout.forward(xx, dropout_param)[0], x, dout)\n","\n","# Error should be around e-10 or less\n","print('dx relative error: ', cs6476.grad.rel_error(dx, dx_num))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"OLzMLx-iwxKs","new_sheet":false,"run_control":{"read_only":false}},"source":["# Fully-connected nets with dropout\n","Test your drop-out implementation with Fully-Connected-NN. Run the following to numerically gradient-check your implementation. You should see errors less than `1e-5`, and different dropout rates should result different error values. We also provide an example code on how you can use `FullyConnectedNet` to run experiments and play with parameters.\n","Note: An implemenation of `adam` optimizer is already provided in the `fully_connected_networks.py` file. Feel free to plug it as an optimizer for testing."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"18ugsX0iwxKu","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet\n","\n","cs6476.utils.reset_seed(6476)\n","\n","N, D, H1, H2, C = 2, 15, 20, 30, 10\n","X = torch.randn(N, D, dtype=torch.float64, device='cpu')#\n","y = torch.randint(C, size=(N,), dtype=torch.int64, device='cpu')\n","\n","for dropout in [0, 0.25, 0.5]:\n","  print('Running check with dropout = ', dropout)\n","  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n","                            weight_scale=5e-2, dropout=dropout,\n","                            seed=0, dtype=torch.float64, device='cpu')\n","\n","  loss, grads = model.loss(X, y)\n","  print('Initial loss: ', loss.item())\n","\n","  # Relative errors should be around e-5 or less.\n","  for name in sorted(grads):\n","    f = lambda _: model.loss(X, y)[0]\n","    grad_num = cs6476.grad.compute_numeric_gradient(f, model.params[name])\n","    print('%s relative error: %.2e' % (name, cs6476.grad.rel_error(grad_num, grads[name])))\n","  print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9LGeJGzA-EX"},"outputs":[],"source":["from fully_connected_networks import FullyConnectedNet\n","\n","# Train two identical nets, one with dropout and one without\n","cs6476.utils.reset_seed(6476)\n","num_train = 40000\n","small_data = {\n","  'X_train': data_dict['X_train'][:num_train],\n","  'y_train': data_dict['y_train'][:num_train],\n","  'X_val': data_dict['X_val'],\n","  'y_val': data_dict['y_val'],\n","}\n","\n","\n","model = FullyConnectedNet([512], dropout=0.5, dtype=torch.float32, device='cpu')\n","\n","solver = Solver(model, small_data,\n","                num_epochs=100, batch_size=512,\n","                update_rule=sgd_momentum,\n","                optim_config={\n","                  'learning_rate': 5e-3,\n","                },\n","                print_every=100000, print_acc_every=10,\n","                verbose=True, device='cpu')\n","solver.train()\n","print(solver)"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"dmhrgg5hwxKy","new_sheet":false,"run_control":{"read_only":false}},"source":["## Optimzation Experiments\n","Congraturaltions you have all the code you need to train your fully connected neural network. Play around with things like network depth, dropout rate, hidden layer width and optimization algorithm to produce the best result. You can use the testing code provided above to set your own experiments. Once you have the optimal parameters create a function `optimal` in the file `fully_connected_networks.py` of the form:\n","```\n","def optimal(data):\n","    model = FullyConnectedNet([256], dropout=0.5, dtype=torch.float32, device='cpu')\n","    solver = Solver(model, data,\n","                    num_epochs=100, batch_size=512,\n","                    update_rule=sgd_momentum,\n","                    optim_config={\n","                    'learning_rate': 5e-3,\n","                    },\n","                    print_every=100000, print_acc_every=10,\n","                    verbose=True, device='cpu')\n","    solver.train()# dont forget to comment this line in final submission after you have trained your model\n","    solver.model.save(path)# dont forget to comment this line in final submision after you have saved your model\n","    return solver\n","```\n","Optimal parameters can be hardcoded. Autograder will use its own data of the format provided in above cell `small_data` for testing. Please save the model as `fcn_model.pth`. Make sure that device is set tp `cpu` in the final submission.\n","\n","```\n","def gtid():\n","    return gtid\n","```\n","also write a function which return your gtid as an integer value. Please note that you have to use your gtid to set seed for all experiments and code. It will be crucial for accurate testing over gradescope"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}