{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CS 6476 Assignment 2 | Part 3: Transfer Learning with ResNet-18"],"metadata":{"id":"qLUHxNGp3G8h"}},{"cell_type":"markdown","source":["## Introduction\n","\n","So far we have looked at using linear models or shallow CNNs to classify images. These models have been somewhat successful, but modern deep neural networks do outperform the previous models by a large margin.\n","\n","The catch is that we would need to train the large model on our data for many epochs to achieve decent performance, which is computationally expensive. Enter transfer learning!\n","\n"],"metadata":{"id":"MeykGX6l3OOR"}},{"cell_type":"markdown","source":["### Aim\n","\n","We will \"fine-tune\" a pretrained ResNet-18 model on CIFAR-10. We will change the final classifier layer in the model and train it on CIFAR-10 dataset."],"metadata":{"id":"aSYxY1RL4BFR"}},{"cell_type":"markdown","source":["### Assignment requirement\n","\n","This part of the assignment will make use of the GPU. Please make sure to use the GPU in Colab. This notebook will require around 15 minutes to execute end-to-end.\n","\n","Turn on GPU: Click on \"Runtime\" -> \"Change runtime type\" -> Choose \"T4 GPU\" under \"Hardware accelerator\" -> Click \"Save\" -> Your runtime will restart with the GPU enabled."],"metadata":{"id":"eCOAlUc57wUi"}},{"cell_type":"markdown","source":["### Grading and submission\n","\n","You will need to add your code in the following sections:\n","- Model definition -- changing final layer\n","- Model training -- hyperparameters\n","\n","Please submit the final trained model file, \"resnet.pth\" to Gradescope along with your other \".py\" files from the earlier sections.\n","\n","**You will be graded on the basis of your model's accuracy on a hidden split of data.**"],"metadata":{"id":"dR1MyQRoBJZf"}},{"cell_type":"markdown","source":["### What is transfer learning?\n","\n","Transfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n","\n","In transfer learning, the knowledge of an already trained machine learning model is applied to a different but related problem. With transfer learning, we basically try to exploit what has been learned in one task to improve generalization in another. We transfer the weights that a network has learned at “task A” to a new “task B.”\n","\n","The general idea is to use the knowledge a model has learned from a task with a lot of available labeled training data in a new task that doesn't have much data. Instead of starting the learning process from scratch, we start with patterns learned from solving a related task.\n","\n","### Fine-tuning\n","\n","Here is a typical workflow of transfer learning:\n","\n","1. Take layers from a previously trained model.\n","2. Add some new layers on top of the pretrained layers. They will learn to turn the old features into predictions on a new dataset.\n","3. Train the new layers on your dataset."],"metadata":{"id":"Bsj1i9fC8qpE"}},{"cell_type":"markdown","source":["In our case, we will use a ResNet-18 trained on the ImageNet-1k dataset containing around 1.3 million labelled images. We will train the final classifier on CIFAR-10."],"metadata":{"id":"LVnWcPsT9hpa"}},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"Q7ymI0aZ2W1b","new_sheet":false,"run_control":{"read_only":false}},"source":["### Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"c_LLpLyC2eau","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"mbq-UT8J2mnv","new_sheet":false,"run_control":{"read_only":false}},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n","\n","```\n","['convolutional_networks.py', 'fully_connected_networks.py', 'a2_helper.py', 'fully_connected_networks.ipynb', 'cs6476', 'convolutional_networks.ipynb']\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"WcrhTOZW243H","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a CV2023 folder and put all the files under A2 folder, then 'CV2023/A2'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None #enter file name as the above example\n","GOOGLE_DRIVE_PATH = os.path.join('/content','drive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"xegb0uDA232J","new_sheet":false,"run_control":{"read_only":false}},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Hello from convolutional_networks.py!\n","Hello from a2_helper.py!\n","```\n","\n","as well as the last edit time for the file `convolutional_networks.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"AhGQF5sw3Fas","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()\n","\n","from convolutional_networks import hello_convolutional_networks\n","hello_convolutional_networks()\n","\n","from a2_helper import hello_helper\n","hello_helper()\n","\n","convolutional_networks_path = os.path.join(GOOGLE_DRIVE_PATH, 'convolutional_networks.py')\n","convolutional_networks_edit_time = time.ctime(os.path.getmtime(convolutional_networks_path))\n","print('convolutional_networks.py last edited on %s' % convolutional_networks_edit_time)"]},{"cell_type":"markdown","source":["## Data loading and preprocessing\n"],"metadata":{"id":"IuqOpj4VBnJv"}},{"cell_type":"markdown","source":["We will load the CIFAR-10, similar to the earlier part."],"metadata":{"id":"ENkNRpvPCFsm"}},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"fN1SShPR4lJV","new_sheet":false,"run_control":{"read_only":false}},"source":["### Imports\n","Here we import some useful packages and increase the default figure size."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"VUCKw4Tl1ddj","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import cs6476\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import statistics\n","import random\n","import time\n","import math\n","%matplotlib inline\n","\n","from cs6476 import reset_seed, Solver\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","plt.rcParams['font.size'] = 16"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"lhqpd2IN2O-K","new_sheet":false,"run_control":{"read_only":false}},"source":["It's necessary to use GPU for this part. Run this cell to make sure you are using a GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"SGDxdBIMRX6b","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["if torch.cuda.is_available():\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"id":"-Yv3zQYw5B3s","new_sheet":false,"run_control":{"read_only":false}},"source":["### Load the CIFAR-10 dataset\n","We have loaded the relevant subsets of CIFAR-10 dataset in the following variables. Note that these are `torch.Tensor`s.\n","\n","- `X_train` contains all training images (real numbers in the range $[0, 1]$)\n","- `y_train` contains all training labels (integers in the range $[0, 9]$)\n","- `X_val` contains all validation images\n","- `y_val` contains all validation labels"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"deletable":true,"id":"V2mFlFmQ1ddm","new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["# Invoke the above function to get our data.\n","cs6476.reset_seed(0)\n","data_dict = cs6476.data.preprocess_cifar10(cuda=False, dtype=torch.float64, flatten=False)\n","X_train = data_dict[\"X_train\"]\n","y_train = data_dict[\"y_train\"]\n","X_val = data_dict[\"X_val\"]\n","y_val = data_dict[\"y_val\"]\n","print('Train data shape: ', data_dict['X_train'].shape)\n","print('Train labels shape: ', data_dict['y_train'].shape)\n","print('Validation data shape: ', data_dict['X_val'].shape)\n","print('Validation labels shape: ', data_dict['y_val'].shape)"]},{"cell_type":"markdown","source":["### Creating Torch Datasets and DataLoader\n","\n","Torch has provided utilities to make batching of data easier: `Dataset` and `DataLoader`. Here, since we have our images as `torch.Tensor`s, we will use an additional utility `TensorDataset`, which will initialize a Torch Dataset from our tensors.\n","\n","\n","We will then create their corresponding `DataLoader`s. These are iterable objects, which can be used to iterate over batches of dataset. We will use them to train the model on batches of data.\n","\n","\n","\n","You can read more on `torch.utils.data.Dataset` [here](https://pytorch.org/docs/stable/data.html)."],"metadata":{"id":"48lpdPAYFzhD"}},{"cell_type":"code","source":["train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=256, drop_last=True)\n","\n","val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=256, drop_last=True)"],"metadata":{"id":"l_xMvuPxBmWJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will test the dataloaders by printing the shape of a single batch. Note that here the first dimension is the batch size. As a rule of thumb, a smaller batch size per compute device (i.e. per GPU) is considered better for optimization."],"metadata":{"id":"0yLEPKKYIbjs"}},{"cell_type":"code","source":["for train_example in train_dataloader:\n","    print(\"Training batch input size: \", train_example[0].shape)\n","    print(\"Training batch output size: \",train_example[1].shape)\n","    break\n","\n","for val_example in val_dataloader:\n","    print(\"Validation batch input size: \", val_example[0].shape)\n","    print(\"Validation batch output size: \",val_example[1].shape)\n","    break"],"metadata":{"id":"Hpbdm_aNHagZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model loading and manipulation\n","\n","In this section, we will load a pretrained ResNet-50 model from `torchvision` and we will replace the last `Linear` layer from the model.\n","\n","See [this page](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) for more details on the pretrained model."],"metadata":{"id":"q5LLy0mOJge_"}},{"cell_type":"markdown","source":["We will replace the `fc` layer from the model and retrain it. Since `torch` is an Pythonic library, we can directly replace the linear layer like an object attribute."],"metadata":{"id":"rU7rCYtQLwO8"}},{"cell_type":"code","source":["def get_model():\n","    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights(torchvision.models.ResNet18_Weights.IMAGENET1K_V1))\n","    ####################################################################\n","    # TODO: Replace the `fc` attribute with a `torch.nn.Linear` layer  #\n","    # with 512 input dimensions and 10 output dimensions.             #\n","    ####################################################################\n","\n","    model.fc = None\n","\n","    ######################################################################\n","    #                            END OF YOUR CODE                        #\n","    ######################################################################\n","    return model"],"metadata":{"id":"2I6WL3pdJEYd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pro tip: you can print the model outline by just running the following cell!"],"metadata":{"id":"lawNPK_jLpi5"}},{"cell_type":"code","source":["model = get_model()\n","model"],"metadata":{"id":"R5De0mn2KYQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will print the summary of the model using `torchsummary` module. We will be able to see the number of trainable weights."],"metadata":{"id":"ATjzX0ehPhwI"}},{"cell_type":"code","source":["from torchsummary import summary\n","\n","summary(model, (3, 32, 32), device=\"cpu\")"],"metadata":{"id":"o44CHSONO9uu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del model # delete the model object so it is not re-used by mistake"],"metadata":{"id":"C6rWBrOlqFQ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune the model"],"metadata":{"id":"vSiTTfbKQbhR"}},{"cell_type":"markdown","source":["The outline of model training procedure is as follows:\n","\n","1. Define hyperparameters -- learning rate and weight decay.\n","2. Initialize optimizer.\n","3. Initialize metrics lists.\n","4. Train the model for as many epochs as you wish.   \n","    a. Iterate over each batch in every epoch.   \n","    b. Calculate loss and update model parameters.   \n","    c. Calculate metrics on train and validation split and store them.\n","\n","\n","See [this page](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) to learn more about `AdamW` optimizer.\n"],"metadata":{"id":"2Dl7YcOrQhqD"}},{"cell_type":"code","source":["######################################################################\n","############################  TODO  ##################################\n","# Define the hyperparameters for training. Play around with them until\n","# you get the best model.\n","######################################################################\n","\n","LEARNING_RATE = None\n","EPOCHS = None\n","WEIGHT_DECAY = None\n","\n","######################################################################\n","#                            END OF YOUR CODE                        #\n","######################################################################"],"metadata":{"id":"-P_yuEYpYIc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = get_model()\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","train_loss_epochwise = list()\n","val_loss_epochwise = list()\n","\n","train_accuracy_epochwise = list()\n","val_accuracy_epochwise = list()"],"metadata":{"id":"6JbsrdHEQSqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def accuracy(y_true, y_pred):\n","    return np.sum(y_true == y_pred) / len(y_true)"],"metadata":{"id":"aFJEEnlnaHmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(EPOCHS):\n","    # Initialize lists for storing losses and outputs. We will use\n","    # these outputs to calculate accuracy.\n","    train_loss_batchwise = list()\n","    val_loss_batchwise = list()\n","\n","    train_predicted_labels = list()\n","    val_predicted_labels = list()\n","\n","    train_labels_list = list()\n","    val_labels_list = list()\n","\n","    model.train() # run before training the model\n","\n","    for images, labels in train_dataloader:\n","        images, labels = images.float().to(device), labels.to(device)\n","        outputs = model(images)\n","\n","        loss = loss_fn(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        train_predicted_labels.append(outputs.argmax(dim=-1).detach().cpu().numpy())\n","        train_labels_list.append(labels.cpu().numpy())\n","\n","        train_loss_batchwise.append(loss.detach().cpu().item())\n","\n","    train_acc = accuracy(np.concatenate(train_labels_list, axis=0), np.concatenate(train_predicted_labels, axis=0))\n","    train_loss = np.mean(train_loss_batchwise)\n","\n","    train_accuracy_epochwise.append(train_acc)\n","    train_loss_epochwise.append(train_loss)\n","\n","    model.eval() # run before evaluating the model\n","\n","    with torch.no_grad():\n","        for images, labels in val_dataloader:\n","            images, labels = images.float().to(device), labels.to(device)\n","            outputs = model(images)\n","\n","            loss = loss_fn(outputs, labels)\n","\n","            val_predicted_labels.append(outputs.argmax(dim=-1).detach().cpu().numpy())\n","            val_loss_batchwise.append(loss.detach().cpu().item())\n","            val_labels_list.append(labels.cpu().numpy())\n","\n","\n","    val_acc = accuracy(np.concatenate(val_labels_list, axis=0), np.concatenate(val_predicted_labels, axis=0))\n","    val_loss = np.mean(val_loss_batchwise)\n","\n","    val_accuracy_epochwise.append(val_acc)\n","    val_loss_epochwise.append(val_loss)\n","\n","    print(\"#\"*20, \"Epoch: \", epoch)\n","    print(\"Train accuracy: \", train_acc)\n","    print(\"Validation accuracy: \", val_acc)\n","\n","    print(\"Train loss: \", train_loss)\n","    print(\"Validation loss: \", val_loss)\n","    print()"],"metadata":{"id":"NvTy_XyGXwzS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Results visualization\n","\n","We will visualize the train and validation metrics for the training run."],"metadata":{"id":"vZuXvgqVqoBm"}},{"cell_type":"code","source":["from matplotlib.pylab import plt\n","from numpy import arange\n","\n","# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(1, EPOCHS + 1)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_loss_epochwise, label='Training Loss')\n","plt.plot(epochs, val_loss_epochwise, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","# Set the tick locations\n","plt.xticks(arange(0, EPOCHS + 1, 5))\n","\n","# Display the plot\n","plt.legend(loc='best')\n","plt.show()"],"metadata":{"id":"Hj8cVB0BqnGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(1, EPOCHS + 1)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_accuracy_epochwise, label='Training Accuracy')\n","plt.plot(epochs, val_accuracy_epochwise, label='Validation Accuracy')\n","\n","# Add in a title and axes labels\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","# Set the tick locations\n","plt.xticks(arange(0, EPOCHS + 1, 5))\n","\n","# Display the plot\n","plt.legend(loc='best')\n","plt.show()"],"metadata":{"id":"cgbh_shCsF1F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create submission file"],"metadata":{"id":"IAXI9qWutrfN"}},{"cell_type":"code","source":["torch.save(model.state_dict(), \"resnet.pth\")"],"metadata":{"id":"err1NVmtttW8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upload this file to Gradescope along with other \".py\" files."],"metadata":{"id":"sJ5hCwuUvRQp"}}]}